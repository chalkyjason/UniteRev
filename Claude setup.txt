Technical Specification and Strategic Roadmap: Multi-Platform Live Protest Discovery Engine
Executive Summary
The digital ecosystem for real-time civil documentation is characterized by severe fragmentation. While the ubiquity of mobile streaming technology has democratized the broadcasting of civil unrest, protests, and rapid-response news events, the consumption of this media remains siloed within proprietary platforms. A user seeking comprehensive situational awareness during a dynamic event in a specific locale must currently navigate a disjointed array of applications—YouTube, Twitch, X (formerly Twitter), and Rumble—each with distinct search syntaxes, algorithmic biases, and discovery limitations.
This report serves as a comprehensive technical specification and architectural roadmap for the "Live Protest Finder," a centralized aggregation engine designed to bridge these divides. The proposed system is not merely a search interface but a sophisticated "Connector-based" metasearch engine that normalizes high-velocity video metadata into a unified, platform-agnostic stream model. The primary engineering challenge addressed herein is the orchestration of data retrieval within strict operational constraints, specifically the restrictive API quotas of YouTube Data API v3 and the lack of standardized developer tooling for emerging platforms like Rumble.
The architectural solution delineated in this document prioritizes a "smart polling" strategy that decouples discovery from validation. By leveraging low-cost signals—such as RSS feeds, user submissions, and allow-listed social media monitoring—the system can achieve near real-time latency without incurring the prohibitive costs associated with high-frequency brute-force searching. The roadmap is structured into four distinct phases, beginning with a robust YouTube-centric Minimum Viable Product (MVP) that introduces novel quota-optimization techniques, followed by the progressive integration of Twitch’s Helix API, signal-based discovery on X, and a verification-driven model for Rumble. Furthermore, the specification includes a detailed breakdown of the database schema, internal API routes, and frontend user experience patterns necessary to deliver a production-grade application for a solo developer, all while adhering to strict privacy guardrails to protect the safety of subjects and broadcasters.
1. Introduction: The Fragmentation of Real-Time Civil Documentation
1.1 The Geopolitical and Technical Context
In the contemporary landscape of civil activism, "live" video has transcended its role as mere content to become a primary source of truth. From the streets of Portland to the boulevards of Paris, the livestream is the unedited, raw documentation of history as it unfolds. However, the platforms that host this content are engineered primarily for entertainment retention rather than geo-temporal situational awareness. Algorithms prioritize watch time and engagement, often burying breaking live content under a deluge of gaming streams, "Just Chatting" broadcasts, or highly produced Video on Demand (VOD) content.
For developers seeking to aggregate this data, the landscape presents a "hostile" technical environment. YouTube offers the highest quality video and metadata but imposes a Draconian daily API quota that effectively bans small developers from continuous searching. Twitch provides excellent real-time developer tools but hosts a content ecosystem where "news" is a needle in a haystack of gaming content. X (formerly Twitter), once the pulse of breaking news, has erected paywalls around its API that make broad keyword listening financially unviable for non-enterprise actors. Rumble, positioning itself as an alternative speech platform, lacks the mature API infrastructure required for programmatic discovery.
1.2 Objective and Scope
The objective of this technical report is to define a concrete, actionable engineering path to building a robust aggregator that solves the "discovery problem" for live protest footage. The system is designed to:
 * Ingest metadata about live video streams from multiple heterogeneous sources.
 * Normalize disparate data structures into a canonical Stream object.
 * Deduplicate content to prevent listing the same event multiple times.
 * Score and Rank streams based on heuristic relevance to the user's intent (e.g., "protest," "rally," "march").
 * Present this unified feed to a client application with sub-minute latency.
The scope of this document encompasses the backend system architecture, relational database design, API consumption strategies, cost modeling, and frontend user interface patterns. It specifically addresses the "Solo Builder" constraint, prioritizing maintainability and low operational overhead.
2. Architectural Paradigm: The Connector Pattern
To manage the complexity of interacting with four distinct third-party platforms, the system architecture enforces a strict separation of concerns through a Connector microservices pattern. This approach decouples platform-specific logic—such as API polling, scraping, HTML parsing, and RSS ingestion—from the core application logic, which handles normalization, ranking, and distribution.
2.1 The Connector Microservices Design
In this architecture, a "Connector" is a self-contained module responsible for all interactions with a specific external platform. This modularity is critical for long-term maintenance. If Twitch deprecates an endpoint or Rumble changes its DOM structure, only the specific Connector module requires updating, while the rest of the application remains stable.
Each Connector is tasked with four primary responsibilities:
 * Authentication Management: The Connector handles the lifecycle of OAuth tokens, API keys, and client secrets. It is responsible for refreshing tokens before they expire and securely storing credentials.
 * Rate Limit Governance: This is the most critical function. The Connector tracks local consumption against the platform's specific quotas (e.g., stopping YouTube requests when the daily limit approaches 9,500 units). It implements "circuit breaker" logic to pause polling if error rates spike.
 * Protocol Translation: External platforms return data in various formats—JSON for YouTube and Twitch, XML for RSS feeds, and potentially raw HTML for scraped content. The Connector parses these raw responses and maps them to the internal canonical format.
 * Error Handling and Backoff: Network failures and 429 (Too Many Requests) errors are inevitable. The Connector implements exponential backoff strategies, ensuring that the system recovers gracefully from outages without spamming the provider.
2.2 The Normalized Stream Model
Central to the interoperability of these diverse platforms is the Normalized Stream Model. Regardless of whether a stream originates from a Twitch WebSocket event, a YouTube RSS feed, or a manual Rumble submission, it must be coerced into a uniform structure before it enters the persistence layer. This allows the frontend to render a "Live Feed" card without needing complex conditional logic for each platform.
The schema for the internal Stream object is defined as follows:
{
  "internal_id": "uuid-v4-string",
  "platform": "youtube | twitch | rumble | x",
  "platform_stream_id": "string (e.g., video_id)",
  "channel_details": {
    "name": "string",
    "id": "string",
    "url": "string",
    "avatar_url": "string",
    "trust_score": "float (0.0 - 1.0)"
  },
  "stream_metadata": {
    "title": "string",
    "description": "string",
    "thumbnail_url": "string",
    "viewer_count": "integer",
    "started_at": "ISO-8601 timestamp",
    "detected_at": "ISO-8601 timestamp",
    "language": "string"
  },
  "status": "LIVE | ENDED | UPCOMING | REMOVED",
  "search_context": {
    "matched_keywords": ["protest", "city_name"],
    "geo_tag": "string (optional)",
    "category": "news | activism"
  }
}

The trust_score field is a derived metric calculated based on the channel's history, verifying whether the source is a known entity (e.g., a verified news outlet) or a new, unverified user. This model facilitates the "Channel Trust Signals" feature requested in the MVP.
2.3 Data Flow: The Dual-Loop Strategy
The system operates on two distinct data processing loops to balance discovery speed with resource conservation:
 * The Discovery Loop (Slow): This process runs every 10–30 minutes, depending on the platform's cost. Its sole purpose is to find new streams that were not previously known to the system. It queries search endpoints, parses RSS feeds, and scans for new user submissions.
 * The Liveness Loop (Fast): This process runs every 60–120 seconds. It iterates through the list of known active streams in the database to update their metadata (viewer count) and check if they are still live. This loop is significantly cheaper because checking the status of a known ID is often a low-cost or cached operation compared to a broad search.
3. Phase 1 Deep Dive: The YouTube Integration Strategy
YouTube is the most valuable source for high-quality, long-form live coverage of civil events. However, it presents the most significant engineering hurdle due to the strict quota limits imposed by the YouTube Data API v3.
3.1 The Quota Mathematical Impossibility
A standard Google Cloud Project is allocated a default quota of 10,000 units per day. Understanding the cost structure is vital:
 * A search request (search.list) costs 100 units.
 * A video details request (videos.list) costs 1 unit.
If a naive implementation were to poll the search.list endpoint for the keyword "protest" every 5 minutes, the math would be:

This exceeds the daily quota by nearly 300%, causing the application to crash and return quotaExceeded errors before the day is half over. This necessitates a "Quota-Aware Architecture" that strictly avoids relying on search.list for the primary polling loop.
3.2 Strategy A: The RSS Backdoor (Discovery Layer)
Research confirms that YouTube generates XML RSS feeds for channels. While there is no direct "global search RSS," specific channels or playlists can be monitored via RSS without incurring API costs. This is the foundation for the "Channel Follow List" MVP feature.
Mechanism:
The system maintains a database of "seed channels"—journalists, independent streamers, and news organizations. The Connector constructs the RSS URL:
https://www.youtube.com/feeds/videos.xml?channel_id={CHANNEL_ID}
Cost: 0 units. This is a standard HTTP GET request.
Process:
 * The backend fetches the XML feed.
 * It parses the <entry> tags to find the latest video ID.
 * It compares this ID against the database. If it's new, it triggers a validation check.
Limitation:
The RSS feed does not reliably distinguish between a standard video upload and a livestream. An entry appears when a video is published. Therefore, the RSS feed acts only as a trigger. It tells the system, "Channel X has new activity," prompting the system to spend 1 API unit to verify if that activity is a livestream.
3.3 Strategy B: The "Surgical Search" (Discovery Layer)
To discover unknown streamers—random citizens who start broadcasting a protest—we must use the search.list endpoint, but we must rate-limit it severely.
Implementation:
The Connector polls search.list with specific parameters:
 * part=snippet
 * eventType=live
 * type=video
 * q=protest|rally|march -gaming (utilizing the exclusion logic requested).
Frequency:
Instead of every 5 minutes, this runs once every 30 minutes.

Viability:
This consumption leaves 5,200 units remaining for other operations, such as checking stream status. While this introduces a latency of up to 30 minutes for discovering a completely unknown streamer, this is an acceptable trade-off for a free-tier MVP.
3.4 Strategy C: The Batch Validation Loop (Liveness Layer)
Once a stream is discovered (via RSS or Search) and added to the "Live" database table, we must monitor it to update viewer counts and detect when it ends. Using videos.list for one video costs 1 unit. However, the API supports batching up to 50 video IDs in a single request.
Optimization:


If we monitor 50 streams every 2 minutes:

This is highly scalable. Even with 500 concurrent live streams, the cost would only be ~7,200 units, which fits within the daily budget when combined with the surgical search strategy.
3.5 Handling "Recently Live" and Archival
The MVP requirement includes a "Recently Live / Uploaded" feed. This is critical for post-event analysis.
Logic:
When the Liveness Loop detects that a stream has ended (the API returns liveStreamingDetails.actualEndTime), the system does not delete the record. Instead:
 * It updates the status from LIVE to ENDED.
 * It records the actual_end_time.
 * It moves the stream from the "Live Now" cache to the "Recent" table.
This data is served via a separate API endpoint (GET /api/feed/recent), sorted by end_time descending. This ensures that users who miss the live event can immediately access the VOD (Video on Demand) if the broadcaster has enabled archiving.
4. Phase 2 Deep Dive: Twitch & Real-Time Discovery
Twitch offers a more modern, developer-friendly API (Helix) with significantly higher rate limits, making it easier to achieve "real-time" responsiveness compared to YouTube. However, the content on Twitch is heavily skewed towards gaming, necessitating robust filtering logic.
4.1 Twitch Helix API Architecture
Twitch uses a "Token Bucket" rate limit system. Standard limits are roughly 800 points per minute. This is orders of magnitude more generous than YouTube's 100 requests per day for search.
4.2 Discovery: Search Channels vs. Get Streams
To find protests, the Twitch Connector utilizes two primary endpoints:
 * Search Channels (GET /search/channels):
   * Usage: The system queries for keywords like "protest", "news", "activism", "police", "rally".
   * Parameter: live_only=true ensures only active broadcasts are returned.
   * Response: Returns a list of channels matching the query that are currently live.
   * Efficiency: This is the most direct method for populating the "Live Now" feed based on textual relevance.
 * Get Streams (GET /streams):
   * Usage: This endpoint allows filtering by game_id (Category). The "Just Chatting" and "News & Politics" categories are where non-gaming content typically resides.
   * Constraint: You cannot search by title keyword efficiently here; you filter by Category IDs.
   * Strategy: The system maps relevant Twitch Category IDs (e.g., "News & Politics" ID 509672) and polls GET /streams?game_id=509672.
4.3 The "Category + Keyword" Filter Strategy
Unlike YouTube, where queries search everything, Twitch is category-centric. The Connector logic for Twitch implements a two-step filter:
 * Step 1 (Category Scan): Poll GET /streams for the "News & Politics" category every 60 seconds. This returns the top 100 streams by viewer count.
 * Step 2 (Client-Side Filtering): Since the API returns the title in the response , the Connector parses the JSON response and applies a Regex filter against the user's keywords (e.g., /protest/i, /rally/i). This filters out unrelated content (e.g., political talk shows) and keeps only on-the-ground footage.
4.4 Caching and Rate Limit Protection
While 800 requests/minute is high, the system must strictly respect the Ratelimit-Remaining headers returned with every response.
Header Logic:
The Connector reads Ratelimit-Remaining. If it drops below a safety threshold (e.g., 50), the system pauses all polling until the timestamp provided in Ratelimit-Reset.
Duplicate Prevention:
Twitch streams often persist across multiple API pages. The system uses a Redis Set to store the stream_ids processed in the current cycle. This avoids redundant database writes or processing if a stream appears in both the "Search" results and the "Category" results.
5. Phase 3 & 4: Signal Intelligence (X & Rumble)
As we move beyond the major APIs, the architecture shifts from "Official API Integration" to "Signal Intelligence" and "User Submission."
5.1 Phase 3: X (Twitter) as a Signal Source
X has transitioned from an open platform to a highly restricted one. The "Free" tier is write-only, and the "Basic" tier ($100/mo) allows only 10,000 reads/month. This makes X unviable as a primary video host but valuable as a discovery vector for links.
The "Allowlist Monitoring" Pattern:
Instead of attempting to search the global firehose (which is cost-prohibitive), the system monitors a curated "Allowlist" of high-value accounts—local journalists, ACLU branches, and known aggregators.
Implementation:
 * Endpoint: users/:id/tweets (Basic Tier).
 * Filter: The Connector scans the text of posts from these accounts for links containing youtube.com/watch, twitch.tv, or rumble.com.
 * Extraction: If a link is found, the system extracts the Video ID and feeds it into the respective YouTube or Twitch Connector for validation.
Impact: This transforms X into a discovery engine for the other platforms, effectively bypassing the limitations of its own video hosting.
5.2 Phase 4: Rumble – The Submission Model
Rumble lacks a public search API for developers. The "Live Stream API" mentioned in their documentation is for broadcasters, not consumers.
The Submission System:
Given the instability of scraping, the most robust architectural decision for Rumble is a User Submission System.
 * Frontend: Users paste a Rumble URL into an "Add Stream" interface.
 * Backend Validation: The server performs a single HTTP GET request to the provided URL.
 * HTML Parsing: The server parses the meta tags (OpenGraph og:video:type or specific DOM elements indicating "Live") to confirm the stream's status.
 * Persistence: Valid streams are added to the DB.
 * Re-validation: A background job checks that specific URL every 5 minutes to verify it is still live.
This decentralizes the discovery cost to the user base while maintaining data integrity through server-side validation.
6. Data Persistence and Schema Design
A robust relational database schema is required to normalize the differences between platforms and support the "Recently Live" and "Follow" features. PostgreSQL is the chosen technology due to its reliability and support for JSONB types.
6.1 Database Tables
The following SQL definitions outline the core schema.
Table: channels
Stores the identity of broadcasters across platforms.
CREATE TABLE channels (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    platform VARCHAR(50) NOT NULL CHECK (platform IN ('youtube', 'twitch', 'rumble', 'x')),
    platform_channel_id VARCHAR(255) NOT NULL,
    display_name VARCHAR(255) NOT NULL,
    avatar_url TEXT,
    trust_score DECIMAL(3, 2) DEFAULT 0.50, -- 0.00 to 1.00
    last_scraped_at TIMESTAMP WITH TIME ZONE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    UNIQUE(platform, platform_channel_id)
);

CREATE INDEX idx_channels_platform ON channels(platform);

Table: streams
The central entity representing a live or archived broadcast.
CREATE TABLE streams (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    channel_id UUID REFERENCES channels(id) ON DELETE CASCADE,
    platform_stream_id VARCHAR(255) NOT NULL, -- e.g., YouTube Video ID
    title TEXT NOT NULL,
    description TEXT,
    thumbnail_url TEXT,
    status VARCHAR(20) CHECK (status IN ('LIVE', 'ENDED', 'UPCOMING', 'REMOVED')),
    viewer_count INTEGER DEFAULT 0,
    start_time TIMESTAMP WITH TIME ZONE,
    end_time TIMESTAMP WITH TIME ZONE, -- Populated when stream ends
    detected_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    keywords JSONB DEFAULT '', -- Array of matched keywords
    geo_city VARCHAR(100), -- Inferred location
    geo_country VARCHAR(100),
    UNIQUE(channel_id, platform_stream_id)
);

-- Compound index for the "Live Now" feed
CREATE INDEX idx_streams_live_rank ON streams(status, viewer_count DESC);
-- Index for the "Recently Live" feed
CREATE INDEX idx_streams_recent ON streams(status, end_time DESC);
-- GIN Index for full-text search on titles
CREATE INDEX idx_streams_title_search ON streams USING GIN(to_tsvector('english', title));

Table: user_follows
Supports the MVP feature for pinning channels.
CREATE TABLE user_follows (
    user_device_id VARCHAR(255) NOT NULL, -- Simple ID for MVP, no auth
    channel_id UUID REFERENCES channels(id) ON DELETE CASCADE,
    created_at TIMESTAMP DEFAULT NOW(),
    PRIMARY KEY (user_device_id, channel_id)
);

6.2 The "Trust Signal" Algorithm
The trust_score in the channels table is a critical relevance signal. It is calculated as follows:
 * Account Age (W_a=0.3): Older accounts are less likely to be spam bots.
 * Subscriber Count (W_s=0.3): Higher counts imply established audiences (though this biases against new citizen journalists).
 * History (W_h=0.4): Have they streamed protests before? (This requires manual tagging or historical data analysis).
For the MVP, this can be simplified: Known "Allowlist" channels get a score of 1.0. Discovered channels start at 0.5.
7. Backend Application Logic & API Specification
The backend serves as the orchestration layer, exposing a RESTful API to the mobile client while managing the background polling jobs.
7.1 Smart Polling Schedule (The "Heartbeat")
To manage thousands of potential channels without exhausting quotas, the backend implements a Dynamic Adaptive Polling engine.
The Algorithm:
 * Assign Priority: Every channel in the database is assigned a polling interval (T) based on its recent activity.
   * High Priority (T=2m): Channel is currently live or went live in the last 24 hours.
   * Medium Priority (T=30m): Channel has streamed in the last 7 days.
   * Low Priority (T=6h): Channel hasn't streamed in 30 days.
   * Dead (T=24h+): Inactive for months.
 * Execution: A background worker (implemented using Python Celery or Node.js Bull) selects channels where last_checked + T < current_time.
 * Batching: The worker groups these channels into batches (50 for YouTube, 100 for Twitch) to minimize HTTP requests.
This ensures that 90% of the API budget is spent on channels that are actually likely to be streaming.
7.2 API Routes (Concrete Spec)
The mobile app interacts with the backend via the following JSON endpoints:
1. GET /api/v1/feed/live
 * Purpose: Returns the main "Live Now" feed.
 * Params: keywords (string, optional), platform (filter), sort (relevance | viewers).
 * Logic: Queries the streams table where status='LIVE'. Applies the weighted ranking score.
2. GET /api/v1/feed/recent
 * Purpose: Returns the "Recently Live / Uploaded" feed.
 * Params: limit (default 20), offset.
 * Logic: Queries streams where status='ENDED' ordered by end_time DESC.
3. GET /api/v1/channels/search
 * Purpose: Allows users to find channels to follow.
 * Params: q (search term).
 * Logic: Performs a fuzzy search on the channels table.
4. POST /api/v1/submit
 * Purpose: User submission of Rumble/Other links.
 * Body: { "url": "https://rumble.com/..." }
 * Logic: Triggers the validation job. Returns 202 Accepted if format is valid.
5. POST /api/v1/user/follow
 * Purpose: "Pin" a channel.
 * Body: { "device_id": "...", "channel_id": "..." }
 * Logic: Inserts into user_follows.
8. Frontend User Experience & Interface Design
The value of the backend is only realized through an effective User Interface (UI). The application prioritizes immediacy and verification.
8.1 Screen 1: The "Live Now" Feed (Home)
This is the landing screen.
 * Header: Search bar with "Keyword Presets" chips (e.g., "Protest", "Police", "Rally", "City Name").
 * List View: A vertical scroll of Stream Cards.
   * Card Design: Large 16:9 thumbnail. A prominent red "LIVE" badge in the top left. Viewer count overlay in the bottom left.
   * Metadata: Stream Title (truncated to 2 lines), Channel Name with a "Verified" tick if trust_score > 0.8, and Platform Icon (YouTube Red, Twitch Purple, Rumble Green).
 * Interaction: Tapping a card opens the stream in a native player or deep-links to the respective app.
8.2 Screen 2: The "Recent" Feed (Archive)
Accessible via a tab at the bottom.
 * Purpose: Allows users to catch up on events they missed.
 * List View: Similar to Home, but the badge says "ENDED • 2h ago" in gray.
 * Sorting: Strictly chronological (newest ended first).
8.3 Screen 3: The "Following" List
 * Purpose: Personalized view of pinned channels.
 * Layout:
   * Top Section: "Live Updates" – Only shows followed channels that are currently live.
   * Bottom Section: "All Followed Channels" – A list of avatars and names with a status indicator (Green dot for live, Gray for offline).
 * Logic: Driven by the user_follows table.
8.4 Screen 4: Submission Interface
Accessible via a Floating Action Button (FAB) "+" on the Home screen.
 * Input: A text field for pasting a URL.
 * Validation: As the user types, regex checks if the URL is from a supported domain.
 * Action: "Submit Stream" button.
 * Feedback: Toast notification: "Stream submitted for verification."
9. Trust, Safety, and Privacy Engineering
Aggregating protest content carries significant ethical and legal responsibilities. The system must be designed to minimize harm to subjects and broadcasters.
9.1 Geo-Fencing and Location Privacy
 * Risk: Precise GPS coordinates can facilitate "swatting" or police targeting of activists.
 * Guardrail: The system never displays or stores precise latitude/longitude data, even if the API provides it.
 * Implementation: All location data is processed through a "Geohash Rounding" algorithm or mapped strictly to City/Region polygons. The UI displays "Seattle, WA," never "3rd & Pine."
9.2 Anti-Doxxing and Content Moderation
 * Automated Filtering: A regex blocklist prevents streams with titles containing sensitive PII (Personally Identifiable Information) like phone numbers or home addresses from appearing in the feed.
 * Reporting Mechanism: Every Stream Card has a "Report" option (hidden behind a generic "Three Dots" menu).
 * Safety Circuit Breaker: If a stream receives >N unique reports within M minutes, it is automatically hidden from the main feed pending administrative review. This "shoot first, ask questions later" approach prioritizes safety.
9.3 Platform Compliance
 * Terms of Service: The app acts as a directory, not a host. It embeds players using the official platform SDKs or linking out. This ensures compliance with YouTube and Twitch Developer Policies regarding view counts and ad impressions.
10. Infrastructure, Cost Modeling, and Scaling
10.1 Infrastructure Costs (Monthly Estimate for MVP)
For a solo builder, minimizing fixed costs is essential.
 * Compute (Backend): 1x DigitalOcean Droplet (Basic, 1GB RAM) or AWS t3.small. Cost: ~$6-$10/mo.
 * Database: Managed PostgreSQL (e.g., Supabase or Neon). Free tier is sufficient for MVP (up to 500MB data). Cost: $0.
 * Cache: Redis (Managed or self-hosted on the Droplet). Cost: $0-$10/mo.
 * YouTube API: Free Tier (Strict adherence to 10k quota). Cost: $0.
 * Twitch API: Free Tier. Cost: $0.
 * Domain & SSL: Cost: ~$1/mo.
 * Total Monthly OpEx: ~$7 - $21.
10.2 Scaling Bottlenecks
 * Database Writes: As the number of streams grows, the "Liveness Loop" creates heavy write traffic (updating viewer counts every minute).
   * Solution: Use Redis for the "Liveness" state (viewer counts) and only write to PostgreSQL on status transitions (Start/End).
 * YouTube Quota: The hard ceiling of 10,000 units cannot be scaled without a quota audit request to Google. This audit requires proving the app adds value to the YouTube ecosystem. The "Batch Validation" strategy is the primary mitigation here, allowing the system to scale to thousands of tracked streams before hitting the limit.
11. Implementation Roadmap (Concrete Steps)
This roadmap assumes a 4-week sprint for a solo developer to reach MVP.
Week 1: Foundation & YouTube
 * Initialize PostgreSQL database with channels and streams tables.
 * Implement the YouTubeConnector with the "RSS Backdoor" logic.
 * Build the "Batch Validation" job using the videos.list endpoint.
 * Write the seed_channels.py script to populate the initial 50 trust-signal channels.
Week 2: Twitch & Polling Engine
 * Implement the TwitchConnector using Helix API.
 * Build the "News & Politics" category poller with keyword filtering.
 * Develop the "Smart Polling" background worker to manage update frequencies.
 * Set up Redis for caching stream states.
Week 3: API & Frontend Skeleton
 * Build the FastAPI/Express endpoints (GET /feed/live, GET /feed/recent).
 * Initialize the React Native (Expo) project.
 * Build the "Stream Card" component and the "Live Feed" screen.
Week 4: Polish, Rumble & Launch
 * Implement the generic "Submission" endpoint and frontend FAB.
 * Add the "Recently Live" logic to move ended streams to the archive view.
 * Implement the "Report" button and moderation logic.
 * Deploy backend to DigitalOcean and submit mobile app to TestFlight.
12. Conclusion
The construction of a "Live Protest Finder" is a complex exercise in API resource management and data normalization. The naive approach of "searching everything" is financially and technically impossible given current platform constraints. However, by adopting the hybrid architecture detailed in this report—leveraging RSS for discovery, batch APIs for validation, and user submissions for the long tail of alternative platforms—it is possible to build a powerful, low-latency tool that fits within the "Solo Builder" constraints. This system fills a critical gap in the information ecosystem, providing transparency and situational awareness when it is needed most, while respecting the privacy and safety of those on the ground.
